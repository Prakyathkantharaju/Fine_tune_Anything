Optimization:
  model_name: 'Salesforce/codegen25-7b-mono'
  tokenizer_name: 'Salesforce/codegen25-7b-mono'
  store_data: True
  epochs: 10
  data_dir: 'data/'
  model_dir: 'models/'
  log_dir: 'logs/'
  wandb: False
  token_length: 512
  #TODO: Fix the context length issues
  eos: True # This is chopped off in the middle of the sequence if the context is too long 

repositories:
  matplotlib: "https://github.com/matplotlib/matplotlib"
  numpy: "https://github.com/numpy/numpy"
  pandas: "https://github.com/pandas-dev/pandas"
  opencv: "https://github.com/opencv/opencv-python"
  scikit-learn: "https://github.com/scikit-learn/scikit-learn"
  pytorch: 'https://github.com/pytorch/pytorch'
  tensorflow: 'https://github.com/tensorflow/tensorflow'
  huggingface: 'https://github.com/huggingface/transformers'
  accelerate: 'https://github.com/huggingface/accelerate'



lora_config:
  r: 8
  target_modules: '\((\w+)\): Linear'
  lora_dropout: 0.01
  bias: none
  task_type: "CAUSAL_LM"

